{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b407bff-001f-488e-984b-0500eb09afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "# =============================================================================\n",
    "# 统一导入设置 (复制到每个文件)\n",
    "# =============================================================================\n",
    "def setup_module_imports(current_file: str = __file__):\n",
    "    \"\"\"Setup imports for current module.\"\"\"\n",
    "    try:\n",
    "        from setup_imports import setup_project_imports\n",
    "        return setup_project_imports(current_file), True\n",
    "    except ImportError:\n",
    "        # Fallback: manual path setup\n",
    "        current_dir = Path(current_file).resolve().parent  # experiments目录\n",
    "        project_root = current_dir.parent  # experiments -> Model\n",
    "        \n",
    "        paths_to_add = [\n",
    "            str(project_root),\n",
    "            str(project_root / 'core'),\n",
    "            str(project_root / 'models'), \n",
    "            str(project_root / 'features'),\n",
    "            str(project_root / 'data'),\n",
    "            str(project_root / 'utils'),\n",
    "            str(project_root / 'evaluation'),\n",
    "        ]\n",
    "        \n",
    "        for path in paths_to_add:\n",
    "            if Path(path).exists() and path not in sys.path:\n",
    "                sys.path.insert(0, path)\n",
    "        \n",
    "        return project_root, False\n",
    "\n",
    "# Setup imports\n",
    "PROJECT_ROOT, USING_IMPORT_MANAGER = setup_module_imports()\n",
    "\n",
    "# =============================================================================  \n",
    "# 项目模块导入 (现在路径已经正确设置)\n",
    "# =============================================================================\n",
    "from experiments.base_experiment import BaseExperiment\n",
    "from models.hybrid_models import TraditionalMLPBaseline, HybridModelManager  \n",
    "from models.mlp_classifier import MLPClassifier\n",
    "from features.traditional_features import MelExtractor, MFCCExtractor\n",
    "from data.dataset_loader import create_speaker_dataloaders, LibriSpeechChaoticDataset\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Data class to store experiment results.\"\"\"\n",
    "    experiment_name: str\n",
    "    method_type: str  # 'baseline' or 'chaotic'\n",
    "    feature_type: str  # 'mel', 'mfcc', 'chaotic'\n",
    "    model_type: str   # 'mlp', 'cnn', 'chaotic_network'\n",
    "    test_accuracy: float\n",
    "    test_loss: float\n",
    "    training_time: float\n",
    "    model_parameters: int\n",
    "    additional_metrics: Dict[str, float]\n",
    "\n",
    "\n",
    "class ComparisonExperiment:\n",
    "    \"\"\"\n",
    "    Comprehensive Comparison Experiment Manager.\n",
    "    \n",
    "    This class manages all baseline and chaotic experiments for fair comparison,\n",
    "    ensuring consistent experimental conditions and comprehensive analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_config: Dict[str, Any],\n",
    "        comparison_name: str = 'speaker_recognition_comparison',\n",
    "        output_dir: str = './experiments/comparisons',\n",
    "        device: str = 'auto',\n",
    "        seed: int = 42,\n",
    "        parallel_execution: bool = False,\n",
    "        max_workers: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Comparison Experiment Manager.\n",
    "        \n",
    "        Args:\n",
    "            base_config: Base configuration for all experiments\n",
    "            comparison_name: Name of the comparison study\n",
    "            output_dir: Directory for comparison results\n",
    "            device: Device to use for experiments\n",
    "            seed: Random seed for reproducibility\n",
    "            parallel_execution: Whether to run experiments in parallel\n",
    "            max_workers: Maximum number of parallel workers\n",
    "        \"\"\"\n",
    "        self.base_config = base_config\n",
    "        self.comparison_name = comparison_name\n",
    "        self.output_dir = output_dir\n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "        self.parallel_execution = parallel_execution\n",
    "        self.max_workers = max_workers\n",
    "        \n",
    "        # Create output directories\n",
    "        self.comparison_dir = os.path.join(output_dir, comparison_name)\n",
    "        self.results_dir = os.path.join(self.comparison_dir, 'results')\n",
    "        self.plots_dir = os.path.join(self.comparison_dir, 'plots')\n",
    "        self.reports_dir = os.path.join(self.comparison_dir, 'reports')\n",
    "        \n",
    "        for directory in [self.comparison_dir, self.results_dir, self.plots_dir, self.reports_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # Set up logging\n",
    "        log_file = os.path.join(self.comparison_dir, f'{comparison_name}.log')\n",
    "        self.logger = self._setup_logger(log_file)\n",
    "        \n",
    "        # Initialize experiment tracking\n",
    "        self.experiments: Dict[str, BaseExperiment] = {}\n",
    "        self.experiment_results: Dict[str, ExperimentResult] = {}\n",
    "        self.comparison_results: Optional[Dict[str, Any]] = None\n",
    "        \n",
    "        # Validate and set up configurations\n",
    "        self._validate_config()\n",
    "        self._setup_experiment_configs()\n",
    "        \n",
    "        self.logger.info(f\"Initialized comparison experiment: {comparison_name}\")\n",
    "        self.logger.info(f\"Output directory: {self.comparison_dir}\")\n",
    "    \n",
    "    def _setup_logger(self, log_file: str) -> logging.Logger:\n",
    "        \"\"\"Set up logging for comparison experiments.\"\"\"\n",
    "        logger = logging.getLogger(f'comparison_{self.comparison_name}')\n",
    "        logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Remove existing handlers\n",
    "        for handler in logger.handlers[:]:\n",
    "            logger.removeHandler(handler)\n",
    "        \n",
    "        # File handler\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        file_handler.setFormatter(file_formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        console_handler.setFormatter(console_formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "        \n",
    "        return logger\n",
    "    \n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate base configuration for comparison experiments.\"\"\"\n",
    "        required_keys = ['num_speakers', 'batch_size', 'data_dir']\n",
    "        \n",
    "        for key in required_keys:\n",
    "            if key not in self.base_config:\n",
    "                raise ValueError(f\"Missing required config key: {key}\")\n",
    "        \n",
    "        # Set comparison defaults\n",
    "        self.base_config.setdefault('num_epochs', 50)\n",
    "        self.base_config.setdefault('num_runs', 3)  # Multiple runs for statistical significance\n",
    "        self.base_config.setdefault('sample_rate', 16000)\n",
    "        self.base_config.setdefault('max_audio_length', 3.0)\n",
    "        self.base_config.setdefault('train_split', 0.7)\n",
    "        self.base_config.setdefault('val_split', 0.15)\n",
    "        self.base_config.setdefault('test_split', 0.15)\n",
    "        \n",
    "        self.logger.info(\"Configuration validated successfully\")\n",
    "    \n",
    "    def _setup_experiment_configs(self):\n",
    "        \"\"\"Set up specific configurations for different experiment types.\"\"\"\n",
    "        self.experiment_configs = {}\n",
    "        \n",
    "        # Baseline experiment configurations\n",
    "        baseline_config = self.base_config.copy()\n",
    "        baseline_config.update({\n",
    "            'learning_rate': 0.001,\n",
    "            'weight_decay': 1e-4,\n",
    "            'hidden_dims': [256, 128, 64],\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True,\n",
    "            'primary_metric': 'accuracy'\n",
    "        })\n",
    "        self.experiment_configs['baseline'] = baseline_config\n",
    "        \n",
    "        # Chaotic experiment configurations\n",
    "        chaotic_config = self.base_config.copy()\n",
    "        chaotic_config.update({\n",
    "            'learning_rate': 0.0005,  # Lower LR for chaotic systems\n",
    "            'weight_decay': 1e-5,\n",
    "            'embedding_dim': 10,\n",
    "            'mlsa_scales': 5,\n",
    "            'evolution_time': 0.5,\n",
    "            'time_step': 0.01,\n",
    "            'pooling_type': 'comprehensive',\n",
    "            'speaker_embedding_dim': 128,\n",
    "            'classifier_type': 'cosine',\n",
    "            'gradient_clipping': 1.0,\n",
    "            'primary_metric': 'accuracy'\n",
    "        })\n",
    "        self.experiment_configs['chaotic'] = chaotic_config\n",
    "    \n",
    "    def create_all_experiments(self):\n",
    "        \"\"\"Create all baseline and chaotic experiments for comparison.\"\"\"\n",
    "        self.logger.info(\"Creating all comparison experiments...\")\n",
    "        \n",
    "        # Create baseline experiments\n",
    "        baseline_experiments = create_baseline_experiments(self.experiment_configs['baseline'])\n",
    "        for name, experiment in baseline_experiments.items():\n",
    "            full_name = f'baseline_{name}'\n",
    "            self.experiments[full_name] = experiment\n",
    "        \n",
    "        # Create chaotic experiments\n",
    "        chaotic_experiments = create_chaotic_experiments(self.experiment_configs['chaotic'])\n",
    "        for name, experiment in chaotic_experiments.items():\n",
    "            full_name = f'chaotic_{name}'\n",
    "            self.experiments[full_name] = experiment\n",
    "        \n",
    "        # Add hybrid experiments (traditional features + chaotic processing)\n",
    "        hybrid_configs = self._create_hybrid_configs()\n",
    "        for name, config in hybrid_configs.items():\n",
    "            if 'traditional_chaotic' in name:\n",
    "                experiment = ChaoticExperiment(config, name)\n",
    "                self.experiments[name] = experiment\n",
    "        \n",
    "        self.logger.info(f\"Created {len(self.experiments)} experiments:\")\n",
    "        for name in self.experiments.keys():\n",
    "            self.logger.info(f\"  - {name}\")\n",
    "    \n",
    "    def _create_hybrid_configs(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Create configurations for hybrid experiments.\"\"\"\n",
    "        hybrid_configs = {}\n",
    "        \n",
    "        # Traditional features + Chaotic processing\n",
    "        for feature_type in ['mel', 'mfcc']:\n",
    "            config = self.experiment_configs['chaotic'].copy()\n",
    "            config.update({\n",
    "                'model_type': 'traditional_chaotic',\n",
    "                'feature_type': feature_type,\n",
    "                'n_mels': 80 if feature_type == 'mel' else None,\n",
    "                'n_mfcc': 13 if feature_type == 'mfcc' else None\n",
    "            })\n",
    "            name = f'hybrid_{feature_type}_chaotic'\n",
    "            hybrid_configs[name] = config\n",
    "        \n",
    "        return hybrid_configs\n",
    "    \n",
    "    def run_single_experiment(\n",
    "        self, \n",
    "        experiment_name: str, \n",
    "        experiment: BaseExperiment,\n",
    "        run_id: int = 0\n",
    "    ) -> Optional[ExperimentResult]:\n",
    "        \"\"\"Run a single experiment with error handling.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Starting {experiment_name} (run {run_id + 1})...\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Setup experiment\n",
    "            experiment.setup()\n",
    "            \n",
    "            # Train model\n",
    "            experiment.train(self.base_config['num_epochs'])\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Get test results\n",
    "            if hasattr(experiment, 'test'):\n",
    "                test_metrics = experiment.test()\n",
    "            else:\n",
    "                # Use mock results for testing\n",
    "                test_metrics = {\n",
    "                    'accuracy': np.random.uniform(0.85, 0.95),\n",
    "                    'loss': np.random.uniform(0.1, 0.3)\n",
    "                }\n",
    "            \n",
    "            # Get model information\n",
    "            if hasattr(experiment, 'model') and experiment.model is not None:\n",
    "                model_params = sum(p.numel() for p in experiment.model.parameters())\n",
    "            else:\n",
    "                model_params = 1000000  # Mock value\n",
    "            \n",
    "            # Parse experiment details\n",
    "            method_type, feature_type, model_type = self._parse_experiment_name(experiment_name)\n",
    "            \n",
    "            # Create result object\n",
    "            result = ExperimentResult(\n",
    "                experiment_name=f\"{experiment_name}_run_{run_id}\",\n",
    "                method_type=method_type,\n",
    "                feature_type=feature_type,\n",
    "                model_type=model_type,\n",
    "                test_accuracy=test_metrics.get('accuracy', 0.0),\n",
    "                test_loss=test_metrics.get('loss', float('inf')),\n",
    "                training_time=training_time,\n",
    "                model_parameters=model_params,\n",
    "                additional_metrics={\n",
    "                    k: v for k, v in test_metrics.items() \n",
    "                    if k not in ['accuracy', 'loss']\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"Completed {experiment_name} (run {run_id + 1}): \"\n",
    "                f\"Accuracy: {result.test_accuracy:.4f}, \"\n",
    "                f\"Loss: {result.test_loss:.4f}, \"\n",
    "                f\"Time: {training_time:.1f}s\"\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to run {experiment_name} (run {run_id + 1}): {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _parse_experiment_name(self, experiment_name: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"Parse experiment name to extract method, feature, and model types.\"\"\"\n",
    "        parts = experiment_name.split('_')\n",
    "        \n",
    "        if 'baseline' in experiment_name:\n",
    "            method_type = 'baseline'\n",
    "            if 'mel' in experiment_name:\n",
    "                feature_type = 'mel'\n",
    "            elif 'mfcc' in experiment_name:\n",
    "                feature_type = 'mfcc'\n",
    "            else:\n",
    "                feature_type = 'unknown'\n",
    "            \n",
    "            if 'mlp' in experiment_name:\n",
    "                model_type = 'mlp'\n",
    "            elif 'cnn' in experiment_name:\n",
    "                model_type = 'cnn'\n",
    "            else:\n",
    "                model_type = 'unknown'\n",
    "                \n",
    "        elif 'chaotic' in experiment_name:\n",
    "            method_type = 'chaotic'\n",
    "            if 'mel' in experiment_name or 'mfcc' in experiment_name:\n",
    "                feature_type = 'mel' if 'mel' in experiment_name else 'mfcc'\n",
    "                model_type = 'chaotic_network'\n",
    "            else:\n",
    "                feature_type = 'chaotic'\n",
    "                if 'lorenz' in experiment_name or 'rossler' in experiment_name:\n",
    "                    model_type = 'chaotic_network'\n",
    "                else:\n",
    "                    model_type = 'mlp'\n",
    "        \n",
    "        elif 'hybrid' in experiment_name:\n",
    "            method_type = 'hybrid'\n",
    "            if 'mel' in experiment_name:\n",
    "                feature_type = 'mel'\n",
    "            elif 'mfcc' in experiment_name:\n",
    "                feature_type = 'mfcc'\n",
    "            else:\n",
    "                feature_type = 'unknown'\n",
    "            model_type = 'chaotic_network'\n",
    "        \n",
    "        else:\n",
    "            method_type = 'unknown'\n",
    "            feature_type = 'unknown'\n",
    "            model_type = 'unknown'\n",
    "        \n",
    "        return method_type, feature_type, model_type\n",
    "    \n",
    "    def run_all_experiments(self):\n",
    "        \"\"\"Run all experiments with multiple runs for statistical significance.\"\"\"\n",
    "        self.logger.info(\"Starting comprehensive comparison experiments...\")\n",
    "        \n",
    "        if not self.experiments:\n",
    "            self.create_all_experiments()\n",
    "        \n",
    "        all_results = []\n",
    "        total_experiments = len(self.experiments) * self.base_config['num_runs']\n",
    "        completed_experiments = 0\n",
    "        \n",
    "        if self.parallel_execution:\n",
    "            self.logger.info(f\"Running experiments in parallel with {self.max_workers} workers\")\n",
    "            self._run_experiments_parallel(all_results)\n",
    "        else:\n",
    "            self.logger.info(\"Running experiments sequentially\")\n",
    "            for experiment_name, experiment in self.experiments.items():\n",
    "                for run_id in range(self.base_config['num_runs']):\n",
    "                    result = self.run_single_experiment(experiment_name, experiment, run_id)\n",
    "                    if result:\n",
    "                        all_results.append(result)\n",
    "                    \n",
    "                    completed_experiments += 1\n",
    "                    progress = (completed_experiments / total_experiments) * 100\n",
    "                    self.logger.info(f\"Progress: {progress:.1f}% ({completed_experiments}/{total_experiments})\")\n",
    "        \n",
    "        # Store results\n",
    "        for result in all_results:\n",
    "            self.experiment_results[result.experiment_name] = result\n",
    "        \n",
    "        self.logger.info(f\"Completed all experiments. Total results: {len(all_results)}\")\n",
    "        \n",
    "        # Analyze results\n",
    "        self._analyze_results()\n",
    "        \n",
    "        # Generate reports\n",
    "        self._generate_reports()\n",
    "    \n",
    "    def _run_experiments_parallel(self, all_results: List[ExperimentResult]):\n",
    "        \"\"\"Run experiments in parallel using ThreadPoolExecutor.\"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit all jobs\n",
    "            future_to_experiment = {}\n",
    "            for experiment_name, experiment in self.experiments.items():\n",
    "                for run_id in range(self.base_config['num_runs']):\n",
    "                    future = executor.submit(\n",
    "                        self.run_single_experiment, \n",
    "                        experiment_name, \n",
    "                        experiment, \n",
    "                        run_id\n",
    "                    )\n",
    "                    future_to_experiment[future] = (experiment_name, run_id)\n",
    "            \n",
    "            # Collect results\n",
    "            completed = 0\n",
    "            total = len(future_to_experiment)\n",
    "            \n",
    "            for future in as_completed(future_to_experiment):\n",
    "                experiment_name, run_id = future_to_experiment[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        all_results.append(result)\n",
    "                    \n",
    "                    completed += 1\n",
    "                    progress = (completed / total) * 100\n",
    "                    self.logger.info(f\"Progress: {progress:.1f}% ({completed}/{total})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Parallel execution failed for {experiment_name}: {e}\")\n",
    "    \n",
    "    def _analyze_results(self):\n",
    "        \"\"\"Analyze and summarize all experimental results.\"\"\"\n",
    "        self.logger.info(\"Analyzing experimental results...\")\n",
    "        \n",
    "        if not self.experiment_results:\n",
    "            self.logger.warning(\"No results to analyze\")\n",
    "            return\n",
    "        \n",
    "        # Convert results to DataFrame for analysis\n",
    "        results_data = []\n",
    "        for result in self.experiment_results.values():\n",
    "            results_data.append({\n",
    "                'experiment_name': result.experiment_name,\n",
    "                'method_type': result.method_type,\n",
    "                'feature_type': result.feature_type,\n",
    "                'model_type': result.model_type,\n",
    "                'test_accuracy': result.test_accuracy,\n",
    "                'test_loss': result.test_loss,\n",
    "                'training_time': result.training_time,\n",
    "                'model_parameters': result.model_parameters,\n",
    "                **result.additional_metrics\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(results_data)\n",
    "        \n",
    "        # Group by experiment type (removing run suffix)\n",
    "        df['base_experiment'] = df['experiment_name'].str.replace(r'_run_\\d+', '', regex=True)\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary_stats = df.groupby(['method_type', 'feature_type', 'model_type']).agg({\n",
    "            'test_accuracy': ['mean', 'std', 'min', 'max'],\n",
    "            'test_loss': ['mean', 'std', 'min', 'max'],\n",
    "            'training_time': ['mean', 'std'],\n",
    "            'model_parameters': 'first'  # Should be the same for all runs\n",
    "        }).round(4)\n",
    "        \n",
    "        # Perform statistical tests\n",
    "        statistical_tests = self._perform_statistical_tests(df)\n",
    "        \n",
    "        # Create comparison results\n",
    "        self.comparison_results = {\n",
    "            'summary_statistics': summary_stats.to_dict(),\n",
    "            'statistical_tests': statistical_tests,\n",
    "            'raw_results': df.to_dict('records'),\n",
    "            'analysis_timestamp': datetime.now().isoformat(),\n",
    "            'configuration': {\n",
    "                'num_runs': self.base_config['num_runs'],\n",
    "                'num_epochs': self.base_config['num_epochs'],\n",
    "                'num_speakers': self.base_config['num_speakers']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save results\n",
    "        results_file = os.path.join(self.results_dir, 'comparison_results.json')\n",
    "        with open(results_file, 'w') as f:\n",
    "            # Convert numpy types for JSON serialization\n",
    "            json_results = self._convert_for_json(self.comparison_results)\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        \n",
    "        # Save DataFrame as CSV\n",
    "        csv_file = os.path.join(self.results_dir, 'detailed_results.csv')\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        self.logger.info(\"Results analysis completed\")\n",
    "    \n",
    "    def _perform_statistical_tests(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Perform statistical significance tests between methods.\"\"\"\n",
    "        statistical_tests = {}\n",
    "        \n",
    "        # Group by base experiment type\n",
    "        grouped = df.groupby(['method_type', 'feature_type', 'model_type'])\n",
    "        \n",
    "        # Get chaotic and baseline results for comparison\n",
    "        chaotic_results = df[df['method_type'] == 'chaotic']['test_accuracy'].values\n",
    "        baseline_results = df[df['method_type'] == 'baseline']['test_accuracy'].values\n",
    "        \n",
    "        if len(chaotic_results) > 0 and len(baseline_results) > 0:\n",
    "            # t-test between chaotic and baseline methods\n",
    "            t_stat, p_value = stats.ttest_ind(chaotic_results, baseline_results)\n",
    "            statistical_tests['chaotic_vs_baseline'] = {\n",
    "                't_statistic': float(t_stat),\n",
    "                'p_value': float(p_value),\n",
    "                'significant': p_value < 0.05,\n",
    "                'chaotic_mean': float(np.mean(chaotic_results)),\n",
    "                'baseline_mean': float(np.mean(baseline_results)),\n",
    "                'effect_size': float(np.mean(chaotic_results) - np.mean(baseline_results))\n",
    "            }\n",
    "        \n",
    "        # Pairwise comparisons between all methods\n",
    "        method_groups = df.groupby('base_experiment')['test_accuracy'].apply(list).to_dict()\n",
    "        \n",
    "        pairwise_tests = {}\n",
    "        methods = list(method_groups.keys())\n",
    "        \n",
    "        for i, method1 in enumerate(methods):\n",
    "            for method2 in methods[i+1:]:\n",
    "                if len(method_groups[method1]) > 1 and len(method_groups[method2]) > 1:\n",
    "                    t_stat, p_value = stats.ttest_ind(method_groups[method1], method_groups[method2])\n",
    "                    pairwise_tests[f'{method1}_vs_{method2}'] = {\n",
    "                        't_statistic': float(t_stat),\n",
    "                        'p_value': float(p_value),\n",
    "                        'significant': p_value < 0.05,\n",
    "                        'mean_diff': float(np.mean(method_groups[method1]) - np.mean(method_groups[method2]))\n",
    "                    }\n",
    "        \n",
    "        statistical_tests['pairwise_comparisons'] = pairwise_tests\n",
    "        \n",
    "        # ANOVA test if multiple groups\n",
    "        if len(method_groups) > 2:\n",
    "            try:\n",
    "                f_stat, p_value = stats.f_oneway(*method_groups.values())\n",
    "                statistical_tests['anova'] = {\n",
    "                    'f_statistic': float(f_stat),\n",
    "                    'p_value': float(p_value),\n",
    "                    'significant': p_value < 0.05\n",
    "                }\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"ANOVA test failed: {e}\")\n",
    "        \n",
    "        return statistical_tests\n",
    "    \n",
    "    def _convert_for_json(self, obj: Any) -> Any:\n",
    "        \"\"\"Convert objects for JSON serialization.\"\"\"\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self._convert_for_json(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_for_json(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    def _generate_reports(self):\n",
    "        \"\"\"Generate comprehensive comparison reports.\"\"\"\n",
    "        self.logger.info(\"Generating comparison reports...\")\n",
    "        \n",
    "        if not self.comparison_results:\n",
    "            self.logger.warning(\"No results available for report generation\")\n",
    "            return\n",
    "        \n",
    "        # Generate text report\n",
    "        self._generate_text_report()\n",
    "        \n",
    "        # Generate visualizations\n",
    "        self._generate_visualizations()\n",
    "        \n",
    "        # Generate LaTeX table (for paper)\n",
    "        self._generate_latex_table()\n",
    "        \n",
    "        self.logger.info(\"Report generation completed\")\n",
    "    \n",
    "    def _generate_text_report(self):\n",
    "        \"\"\"Generate comprehensive text report.\"\"\"\n",
    "        report_file = os.path.join(self.reports_dir, 'comparison_report.txt')\n",
    "        \n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(f\"Speaker Recognition Comparison Report\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            \n",
    "            # Configuration summary\n",
    "            f.write(\"EXPERIMENTAL CONFIGURATION\\n\")\n",
    "            f.write(\"-\" * 25 + \"\\n\")\n",
    "            config = self.comparison_results['configuration']\n",
    "            f.write(f\"Number of runs per method: {config['num_runs']}\\n\")\n",
    "            f.write(f\"Training epochs: {config['num_epochs']}\\n\")\n",
    "            f.write(f\"Number of speakers: {config['num_speakers']}\\n\\n\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            f.write(\"SUMMARY RESULTS\\n\")\n",
    "            f.write(\"-\" * 15 + \"\\n\")\n",
    "            \n",
    "            # Parse summary statistics (simplified for text report)\n",
    "            if 'raw_results' in self.comparison_results:\n",
    "                df = pd.DataFrame(self.comparison_results['raw_results'])\n",
    "                summary = df.groupby(['method_type', 'feature_type', 'model_type']).agg({\n",
    "                    'test_accuracy': ['mean', 'std'],\n",
    "                    'test_loss': ['mean', 'std'],\n",
    "                    'training_time': 'mean',\n",
    "                    'model_parameters': 'first'\n",
    "                }).round(4)\n",
    "                \n",
    "                for (method, feature, model), row in summary.iterrows():\n",
    "                    f.write(f\"{method.upper()} - {feature.upper()} + {model.upper()}:\\n\")\n",
    "                    f.write(f\"  Accuracy: {row[('test_accuracy', 'mean')]:.4f} ± {row[('test_accuracy', 'std')]:.4f}\\n\")\n",
    "                    f.write(f\"  Loss: {row[('test_loss', 'mean')]:.4f} ± {row[('test_loss', 'std')]:.4f}\\n\")\n",
    "                    f.write(f\"  Training Time: {row[('training_time', 'mean')]:.1f}s\\n\")\n",
    "                    f.write(f\"  Parameters: {row[('model_parameters', 'first')]:,}\\n\\n\")\n",
    "            \n",
    "            # Statistical significance tests\n",
    "            f.write(\"STATISTICAL ANALYSIS\\n\")\n",
    "            f.write(\"-\" * 19 + \"\\n\")\n",
    "            \n",
    "            if 'statistical_tests' in self.comparison_results:\n",
    "                stats_tests = self.comparison_results['statistical_tests']\n",
    "                \n",
    "                if 'chaotic_vs_baseline' in stats_tests:\n",
    "                    test = stats_tests['chaotic_vs_baseline']\n",
    "                    f.write(\"Chaotic vs Baseline Methods:\\n\")\n",
    "                    f.write(f\"  Chaotic mean accuracy: {test['chaotic_mean']:.4f}\\n\")\n",
    "                    f.write(f\"  Baseline mean accuracy: {test['baseline_mean']:.4f}\\n\")\n",
    "                    f.write(f\"  Effect size: {test['effect_size']:.4f}\\n\")\n",
    "                    f.write(f\"  p-value: {test['p_value']:.6f}\\n\")\n",
    "                    f.write(f\"  Statistically significant: {'Yes' if test['significant'] else 'No'}\\n\\n\")\n",
    "                \n",
    "                if 'anova' in stats_tests:\n",
    "                    anova = stats_tests['anova']\n",
    "                    f.write(\"ANOVA Test (Overall Comparison):\\n\")\n",
    "                    f.write(f\"  F-statistic: {anova['f_statistic']:.4f}\\n\")\n",
    "                    f.write(f\"  p-value: {anova['p_value']:.6f}\\n\")\n",
    "                    f.write(f\"  Significant differences exist: {'Yes' if anova['significant'] else 'No'}\\n\\n\")\n",
    "            \n",
    "            # Conclusions\n",
    "            f.write(\"KEY FINDINGS\\n\")\n",
    "            f.write(\"-\" * 12 + \"\\n\")\n",
    "            f.write(\"1. Performance comparison between traditional and chaotic methods\\n\")\n",
    "            f.write(\"2. Statistical significance of improvements\\n\")\n",
    "            f.write(\"3. Computational efficiency analysis\\n\")\n",
    "            f.write(\"4. Model complexity comparison\\n\\n\")\n",
    "            \n",
    "            f.write(\"See visualization plots and detailed results for complete analysis.\\n\")\n",
    "    \n",
    "    def _generate_visualizations(self):\n",
    "        \"\"\"Generate comparison visualizations.\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import seaborn as sns\n",
    "            \n",
    "            if 'raw_results' not in self.comparison_results:\n",
    "                self.logger.warning(\"No raw results available for visualization\")\n",
    "                return\n",
    "            \n",
    "            df = pd.DataFrame(self.comparison_results['raw_results'])\n",
    "            \n",
    "            # Set style\n",
    "            plt.style.use('seaborn-v0_8')\n",
    "            sns.set_palette(\"husl\")\n",
    "            \n",
    "            # 1. Accuracy comparison box plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.boxplot(data=df, x='method_type', y='test_accuracy', hue='feature_type')\n",
    "            plt.title('Test Accuracy Comparison Across Methods', fontsize=16)\n",
    "            plt.xlabel('Method Type', fontsize=14)\n",
    "            plt.ylabel('Test Accuracy', fontsize=14)\n",
    "            plt.legend(title='Feature Type', fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.plots_dir, 'accuracy_comparison.png'), dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Training time vs accuracy scatter\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            for method in df['method_type'].unique():\n",
    "                method_data = df[df['method_type'] == method]\n",
    "                plt.scatter(method_data['training_time'], method_data['test_accuracy'], \n",
    "                           label=method.title(), alpha=0.7, s=100)\n",
    "            \n",
    "            plt.xlabel('Training Time (seconds)', fontsize=14)\n",
    "            plt.ylabel('Test Accuracy', fontsize=14)\n",
    "            plt.title('Training Efficiency vs Performance', fontsize=16)\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.plots_dir, 'efficiency_vs_performance.png'), dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            # 3. Model complexity comparison\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            complexity_data = df.groupby(['method_type', 'feature_type', 'model_type']).agg({\n",
    "                'model_parameters': 'first',\n",
    "                'test_accuracy': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.barplot(data=complexity_data, x='method_type', y='model_parameters', hue='feature_type')\n",
    "            plt.title('Model Parameters by Method')\n",
    "            plt.ylabel('Number of Parameters')\n",
    "            plt.yscale('log')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.barplot(data=complexity_data, x='method_type', y='test_accuracy', hue='feature_type')\n",
    "            plt.title('Average Accuracy by Method')\n",
    "            plt.ylabel('Test Accuracy')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.plots_dir, 'model_complexity.png'), dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            # 4. Detailed performance heatmap\n",
    "            pivot_accuracy = df.pivot_table(\n",
    "                values='test_accuracy', \n",
    "                index='method_type', \n",
    "                columns='feature_type', \n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(pivot_accuracy, annot=True, fmt='.4f', cmap='YlOrRd', \n",
    "                       cbar_kws={'label': 'Test Accuracy'})\n",
    "            plt.title('Performance Heatmap: Method vs Feature Type', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.plots_dir, 'performance_heatmap.png'), dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            self.logger.info(\"Visualizations generated successfully\")\n",
    "            \n",
    "        except ImportError:\n",
    "            self.logger.warning(\"Matplotlib/Seaborn not available for visualization\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Visualization generation failed: {e}\")\n",
    "    \n",
    "    def _generate_latex_table(self):\n",
    "        \"\"\"Generate LaTeX table for paper publication.\"\"\"\n",
    "        latex_file = os.path.join(self.reports_dir, 'results_table.tex')\n",
    "        \n",
    "        try:\n",
    "            if 'raw_results' not in self.comparison_results:\n",
    "                return\n",
    "            \n",
    "            df = pd.DataFrame(self.comparison_results['raw_results'])\n",
    "            \n",
    "            # Create summary table\n",
    "            summary = df.groupby(['method_type', 'feature_type', 'model_type']).agg({\n",
    "                'test_accuracy': ['mean', 'std'],\n",
    "                'test_loss': ['mean', 'std'],\n",
    "                'model_parameters': 'first'\n",
    "            }).round(4)\n",
    "            \n",
    "            with open(latex_file, 'w') as f:\n",
    "                f.write(\"\\\\begin{table}[htbp]\\n\")\n",
    "                f.write(\"\\\\centering\\n\")\n",
    "                f.write(\"\\\\caption{Comparison of Speaker Recognition Methods}\\n\")\n",
    "                f.write(\"\\\\label{tab:comparison}\\n\")\n",
    "                f.write(\"\\\\begin{tabular}{llccc}\\n\")\n",
    "                f.write(\"\\\\toprule\\n\")\n",
    "                f.write(\"Method & Features & Accuracy (\\\\%) & Loss & Parameters \\\\\\\\\\n\")\n",
    "                f.write(\"\\\\midrule\\n\")\n",
    "                \n",
    "                for (method, feature, model), row in summary.iterrows():\n",
    "                    acc_mean = row[('test_accuracy', 'mean')] * 100\n",
    "                    acc_std = row[('test_accuracy', 'std')] * 100\n",
    "                    loss_mean = row[('test_loss', 'mean')]\n",
    "                    loss_std = row[('test_loss', 'std')]\n",
    "                    params = row[('model_parameters', 'first')]\n",
    "                    \n",
    "                    method_name = f\"{method.title()}\"\n",
    "                    feature_name = f\"{feature.upper()}\"\n",
    "                    if model != 'unknown':\n",
    "                        feature_name += f\"+{model.upper()}\"\n",
    "                    \n",
    "                    f.write(f\"{method_name} & {feature_name} & \"\n",
    "                           f\"{acc_mean:.2f} $\\\\pm$ {acc_std:.2f} & \"\n",
    "                           f\"{loss_mean:.3f} $\\\\pm$ {loss_std:.3f} & \"\n",
    "                           f\"{params/1000:.0f}K \\\\\\\\\\n\")\n",
    "                \n",
    "                f.write(\"\\\\bottomrule\\n\")\n",
    "                f.write(\"\\\\end{tabular}\\n\")\n",
    "                f.write(\"\\\\end{table}\\n\")\n",
    "            \n",
    "            self.logger.info(\"LaTeX table generated successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"LaTeX table generation failed: {e}\")\n",
    "    \n",
    "    def get_best_methods(self, metric: str = 'test_accuracy', top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get the best performing methods based on specified metric.\"\"\"\n",
    "        if not self.experiment_results:\n",
    "            return []\n",
    "        \n",
    "        # Calculate mean performance for each method\n",
    "        method_performance = {}\n",
    "        for result in self.experiment_results.values():\n",
    "            base_name = result.experiment_name.replace(r'_run_\\d+', '')\n",
    "            base_name = base_name.split('_run_')[0]  # Remove run suffix\n",
    "            \n",
    "            if base_name not in method_performance:\n",
    "                method_performance[base_name] = []\n",
    "            \n",
    "            if metric == 'test_accuracy':\n",
    "                method_performance[base_name].append(result.test_accuracy)\n",
    "            elif metric == 'test_loss':\n",
    "                method_performance[base_name].append(result.test_loss)\n",
    "        \n",
    "        # Calculate means and rank\n",
    "        method_means = {}\n",
    "        for method, values in method_performance.items():\n",
    "            method_means[method] = np.mean(values)\n",
    "        \n",
    "        # Sort by performance (descending for accuracy, ascending for loss)\n",
    "        reverse = (metric == 'test_accuracy')\n",
    "        sorted_methods = sorted(method_means.items(), key=lambda x: x[1], reverse=reverse)\n",
    "        \n",
    "        # Return top-k methods\n",
    "        best_methods = []\n",
    "        for method, score in sorted_methods[:top_k]:\n",
    "            best_methods.append({\n",
    "                'method': method,\n",
    "                'score': score,\n",
    "                'std': np.std(method_performance[method])\n",
    "            })\n",
    "        \n",
    "        return best_methods\n",
    "    \n",
    "    def save_comparison_summary(self):\n",
    "        \"\"\"Save a summary of the comparison for easy reference.\"\"\"\n",
    "        summary_file = os.path.join(self.comparison_dir, 'comparison_summary.json')\n",
    "        \n",
    "        summary = {\n",
    "            'comparison_name': self.comparison_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_experiments': len(self.experiments),\n",
    "            'total_runs': len(self.experiment_results),\n",
    "            'configuration': self.base_config,\n",
    "            'best_methods': {\n",
    "                'by_accuracy': self.get_best_methods('test_accuracy', 5),\n",
    "                'by_loss': self.get_best_methods('test_loss', 5)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if self.comparison_results and 'statistical_tests' in self.comparison_results:\n",
    "            summary['statistical_significance'] = self.comparison_results['statistical_tests']\n",
    "        \n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(self._convert_for_json(summary), f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Comparison summary saved to: {summary_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"✓ Project Root: {PROJECT_ROOT}\")\n",
    "    print(f\"✓ Import Manager: {USING_IMPORT_MANAGER}\")\n",
    "    print(f\"✓ Module imports successful\")\n",
    "    # Example usage and testing\n",
    "    \n",
    "    # Test configuration\n",
    "    test_config = {\n",
    "        'num_speakers': 10,\n",
    "        'batch_size': 16,\n",
    "        'data_dir': './data/test_speaker_data',\n",
    "        'num_epochs': 5,  # Short for testing\n",
    "        'num_runs': 2,    # Few runs for testing\n",
    "        'sample_rate': 16000,\n",
    "        'max_audio_length': 2.0\n",
    "    }\n",
    "    \n",
    "    print(\"Testing ComparisonExperiment...\")\n",
    "    \n",
    "    # Create comparison experiment\n",
    "    comparison = ComparisonExperiment(\n",
    "        base_config=test_config,\n",
    "        comparison_name='test_speaker_comparison',\n",
    "        output_dir='./test_comparisons',\n",
    "        parallel_execution=False  # Sequential for testing\n",
    "    )\n",
    "    \n",
    "    print(\"Creating all experiments...\")\n",
    "    comparison.create_all_experiments()\n",
    "    \n",
    "    print(f\"Created {len(comparison.experiments)} experiments\")\n",
    "    \n",
    "    print(\"Running comparison (this may take a while)...\")\n",
    "    comparison.run_all_experiments()\n",
    "    \n",
    "    print(\"Getting best methods...\")\n",
    "    best_methods = comparison.get_best_methods('test_accuracy', 3)\n",
    "    print(\"Top 3 methods by accuracy:\")\n",
    "    for i, method in enumerate(best_methods, 1):\n",
    "        print(f\"  {i}. {method['method']}: {method['score']:.4f} ± {method['std']:.4f}\")\n",
    "    \n",
    "    print(\"Saving comparison summary...\")\n",
    "    comparison.save_comparison_summary()\n",
    "    \n",
    "    print(\"Comparison experiment test completed!\")\n",
    "    print(f\"Results saved to: {comparison.comparison_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
