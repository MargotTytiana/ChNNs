# Base Configuration Template
# experiments/configs/base_config.yaml

experiment:
  name: "chaotic_speaker_recognition"
  description: "Robust Speaker Recognition Using Chaotic Neural Networks"
  version: "1.0.0"
  tags: ["speaker_recognition", "chaos_theory", "neural_networks"]
  
# Reproducibility settings
reproducibility:
  seed: 42
  strict_mode: true
  deterministic_algorithms: true
  
# Data configuration
data:
  # Dataset paths
  dataset_path: "../dataset/train-clean-100/LibriSpeech/train-clean-100/"
  sample_rate: 16000
  audio_format: "wav"
  
  # Data splitting
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  
  # Audio preprocessing
  audio_preprocessing:
    normalize: true
    remove_silence: true
    min_duration: 1.0  # seconds
    max_duration: 10.0  # seconds
    apply_noise_reduction: false
    
  # Data augmentation
  augmentation:
    enabled: true
    time_stretch: [0.9, 1.1]
    pitch_shift: [-2, 2]  # semitones
    noise_injection: 0.01
    volume_change: [0.8, 1.2]

# Feature extraction configuration
features:
  # Traditional features for baseline comparison
  traditional:
    mel_spectrogram:
      n_mels: 128
      n_fft: 2048
      hop_length: 512
      win_length: 2048
      f_min: 0
      f_max: 8000
      
    mfcc:
      n_mfcc: 13
      n_fft: 2048
      hop_length: 512
      win_length: 2048
      n_mels: 128
      f_min: 0
      f_max: 8000
      delta: true
      delta_delta: true
  
  # Chaotic features configuration
  chaotic:
    # Phase space reconstruction
    phase_space:
      embedding_dimension: null  # Will be determined by FNN method
      time_delay: null  # Will be determined by autocorrelation
      fnn_rtol: 15.0
      fnn_atol: 2.0
      max_embedding_dim: 10
      
    # Multi-scale Lyapunov Spectrum Analysis
    mlsa:
      scales: [1, 2, 4, 8, 16]
      max_lyapunov_steps: 1000
      dt: 0.01
      jacobian_method: "finite_difference"
      qr_iterations: 100
      
    # Recurrence Quantification Analysis
    rqa:
      recurrence_threshold: 0.1
      min_diagonal_length: 2
      min_vertical_length: 2
      distance_metric: "euclidean"
      normalize_distance: true

# Model configuration
model:
  # Model type selection
  type: "chaotic_network"  # Options: "chaotic_network", "mlp_classifier", "hybrid"
  
  # Chaotic network specific configuration
  chaotic_network:
    # Chaotic embedding layer
    embedding:
      system_type: "lorenz"  # Options: "lorenz", "rossler", "mackey_glass"
      evolution_time: 0.5  # seconds
      time_step: 0.01
      coupling_strength: 1.0
      
      # Lorenz system parameters
      lorenz_params:
        sigma: 10.0
        rho: 28.0
        beta: 2.667
        
      # Alternative system parameters
      rossler_params:
        a: 0.2
        b: 0.2
        c: 5.7
        
      mackey_glass_params:
        beta: 0.2
        gamma: 0.1
        n: 10
        tau: 17
    
    # Strange attractor pooling
    attractor_pooling:
      correlation_dimension:
        min_radius: 0.001
        max_radius: 1.0
        num_radii: 50
        
      lyapunov_dimension:
        calculate: true
        
      kolmogorov_entropy:
        calculate: true
        
    # Speaker embedding layer
    speaker_embedding:
      embedding_dim: 256
      hidden_dims: [512, 256, 128]
      activation: "relu"
      dropout: 0.3
      batch_norm: true
      
    # Classification head
    classifier:
      num_speakers: 100  # Will be set based on dataset
      hidden_dims: [128, 64]
      activation: "relu"
      dropout: 0.5
  
  # MLP baseline configuration
  mlp_classifier:
    input_dim: null  # Will be set based on features
    hidden_dims: [512, 256, 128, 64]
    output_dim: 100  # Number of speakers
    activation: "relu"
    dropout: 0.4
    batch_norm: true
    residual_connections: false

# Training configuration
training:
  # Basic training parameters
  batch_size: 32
  num_epochs: 100
  learning_rate: 0.001
  weight_decay: 1e-5
  
  # Optimizer configuration
  optimizer:
    type: "adam"  # Options: "adam", "sgd", "adamw"
    adam:
      betas: [0.9, 0.999]
      eps: 1e-8
    sgd:
      momentum: 0.9
      nesterov: true
    adamw:
      betas: [0.9, 0.999]
      eps: 1e-8
  
  # Learning rate scheduling
  scheduler:
    type: "cosine_annealing"  # Options: "cosine_annealing", "step", "exponential", "plateau"
    cosine_annealing:
      T_max: 100
      eta_min: 1e-6
    step:
      step_size: 30
      gamma: 0.1
    exponential:
      gamma: 0.95
    plateau:
      mode: "min"
      factor: 0.5
      patience: 10
      min_lr: 1e-6
  
  # Loss function
  loss:
    type: "cross_entropy"  # Options: "cross_entropy", "focal_loss", "label_smoothing"
    focal_loss:
      alpha: 1.0
      gamma: 2.0
    label_smoothing:
      smoothing: 0.1
  
  # Gradient clipping
  gradient_clipping:
    enabled: true
    max_norm: 1.0
  
  # Mixed precision training
  mixed_precision:
    enabled: false
    loss_scale: "dynamic"

# Evaluation configuration
evaluation:
  # Metrics to compute
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
    - "confusion_matrix"
    - "top_k_accuracy"  # k=[1, 3, 5]
  
  # Evaluation frequency
  eval_frequency: 1  # Evaluate every N epochs
  save_predictions: true
  save_attention_weights: false  # For visualization

# Logging configuration
logging:
  # Log levels
  console_level: "INFO"
  file_level: "DEBUG"
  
  # Experiment tracking
  track_gradients: true
  track_weights: true
  log_frequency: 10  # Log every N batches
  
  # Visualization
  visualize_features: true
  visualize_attractors: true
  save_plots: true

# Hardware configuration
hardware:
  # Device selection
  device: "auto"  # Options: "auto", "cpu", "cuda", "cuda:0", etc.
  
  # Multi-GPU training
  multi_gpu:
    enabled: false
    strategy: "data_parallel"  # Options: "data_parallel", "distributed"
  
  # Memory optimization
  memory_optimization:
    gradient_checkpointing: false
    pin_memory: true
    num_workers: 4
    prefetch_factor: 2

# Checkpoint configuration
checkpointing:
  # Save frequency
  save_frequency: 5  # Save every N epochs
  max_checkpoints: 5
  
  # Best model tracking
  auto_save_best: true
  metric_for_best: "val_accuracy"
  metric_mode: "max"
  
  # Checkpoint content
  save_optimizer: true
  save_scheduler: true
  save_full_model: false  # Save only state dict

# Paths configuration
paths:
  # Data paths
  data_dir: "data"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  output_dir: "outputs"
  
  # Model paths
  pretrained_model: null
  resume_from_checkpoint: null
