{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953a3f8-cede-4ae3-8092-c48fdc4f1eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "加载数据集...\n",
      "处理24个演员目录...\n",
      "数据集加载完成: 1440个样本, 8种情感\n",
      "情感分布:\n",
      "  calm: 192个样本\n",
      "  sad: 192个样本\n",
      "  neutral: 96个样本\n",
      "  disgust: 192个样本\n",
      "  surprised: 192个样本\n",
      "  angry: 192个样本\n",
      "  fearful: 192个样本\n",
      "  happy: 192个样本\n",
      "\n",
      "==================================================\n",
      "训练混沌神经网络\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yotta\\.conda\\envs\\VirEnvir\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# 配置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 数据预处理类\n",
    "class SpeechEmotionDataset(Dataset):\n",
    "    def __init__(self, base_path, max_length=500, n_mfcc=40, actor_ids=None):\n",
    "        \"\"\"\n",
    "        :param base_path: 数据集根目录 (e.g., 'F:/F/LifeLongLearning/TUNI/Thesis/Code/archive')\n",
    "        :param max_length: MFCC序列最大长度\n",
    "        :param n_mfcc: MFCC特征维度\n",
    "        :param actor_ids: 使用的演员ID列表 (None表示使用全部)\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.max_length = max_length\n",
    "        self.n_mfcc = n_mfcc\n",
    "        \n",
    "        # 情感标签映射\n",
    "        emotion_map = {\n",
    "            '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "            '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "        }\n",
    "        \n",
    "        # 获取所有演员目录\n",
    "        if actor_ids is None:\n",
    "            actor_dirs = glob.glob(os.path.join(base_path, 'Actor_*'))\n",
    "        else:\n",
    "            actor_dirs = [os.path.join(base_path, f'Actor_{id:02d}') for id in actor_ids]\n",
    "        \n",
    "        print(f\"处理{len(actor_dirs)}个演员目录...\")\n",
    "        \n",
    "        # 遍历所有演员目录\n",
    "        for actor_dir in actor_dirs:\n",
    "            if not os.path.isdir(actor_dir):\n",
    "                continue\n",
    "                \n",
    "            # 获取该演员的所有wav文件\n",
    "            wav_files = glob.glob(os.path.join(actor_dir, '*.wav'))\n",
    "            \n",
    "            for wav_file in wav_files:\n",
    "                # 提取文件名并解析情感\n",
    "                filename = os.path.basename(wav_file)\n",
    "                parts = filename.split('-')\n",
    "                \n",
    "                # 确保文件名格式正确\n",
    "                if len(parts) < 7:\n",
    "                    continue\n",
    "                \n",
    "                emotion_code = parts[2]\n",
    "                emotion = emotion_map.get(emotion_code, None)\n",
    "                \n",
    "                if emotion is None:\n",
    "                    continue\n",
    "                \n",
    "                # 加载音频并提取MFCC特征\n",
    "                try:\n",
    "                    y, sr = librosa.load(wav_file, sr=16000)\n",
    "                    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "                    \n",
    "                    # 标准化并填充/截断到固定长度\n",
    "                    mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
    "                    if mfcc.shape[1] > max_length:\n",
    "                        mfcc = mfcc[:, :max_length]\n",
    "                    else:\n",
    "                        pad_width = max_length - mfcc.shape[1]\n",
    "                        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                    \n",
    "                    # 添加到数据集\n",
    "                    self.data.append(mfcc)\n",
    "                    self.labels.append(emotion)\n",
    "                except Exception as e:\n",
    "                    print(f\"处理文件 {wav_file} 时出错: {str(e)}\")\n",
    "        \n",
    "        # 将标签转换为数字\n",
    "        self.emotion_to_idx = {e: i for i, e in enumerate(set(self.labels))}\n",
    "        self.labels_idx = [self.emotion_to_idx[l] for l in self.labels]\n",
    "        self.num_classes = len(self.emotion_to_idx)\n",
    "        \n",
    "        print(f\"数据集加载完成: {len(self.data)}个样本, {self.num_classes}种情感\")\n",
    "        print(\"情感分布:\")\n",
    "        for emotion, idx in self.emotion_to_idx.items():\n",
    "            count = self.labels.count(emotion)\n",
    "            print(f\"  {emotion}: {count}个样本\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels_idx[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# 1. Rossler混沌神经网络\n",
    "class RosslerCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, a=0.2, b=0.2, c=5.7, dt=0.1):\n",
    "        super(RosslerCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.a = nn.Parameter(torch.tensor(a, dtype=torch.float32))\n",
    "        self.b = nn.Parameter(torch.tensor(b, dtype=torch.float32))\n",
    "        self.c = nn.Parameter(torch.tensor(c, dtype=torch.float32))\n",
    "        self.dt = dt\n",
    "        \n",
    "        # 输入到隐藏状态的权重\n",
    "        self.w_ih = nn.Linear(input_size, 3 * hidden_size)\n",
    "        \n",
    "        # 隐藏状态到隐藏状态的权重\n",
    "        self.w_hh = nn.Linear(3 * hidden_size, 3 * hidden_size)\n",
    "        \n",
    "        # Lyapunov指数监控\n",
    "        self.le_reg = LyapunovRegularizer(target_le=0.05)\n",
    "        \n",
    "        # 初始化隐藏状态\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"初始化权重\"\"\"\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            if weight.dim() > 1:\n",
    "                weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, hx=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 初始化隐藏状态\n",
    "        if hx is None:\n",
    "            hx = torch.zeros(batch_size, 3, self.hidden_size, device=x.device)\n",
    "            hx += 0.01 * torch.randn_like(hx)\n",
    "        \n",
    "        # 分离状态分量\n",
    "        x_state, y_state, z_state = torch.chunk(hx, 3, dim=1)\n",
    "        x_state = x_state.squeeze(1)\n",
    "        y_state = y_state.squeeze(1)\n",
    "        z_state = z_state.squeeze(1)\n",
    "        \n",
    "        # 输入变换\n",
    "        i2h = self.w_ih(x)  # (batch_size, 3*hidden_size)\n",
    "        i2h = i2h.view(batch_size, 3, self.hidden_size)\n",
    "        i_x, i_y, i_z = torch.chunk(i2h, 3, dim=1)\n",
    "        \n",
    "        # Rossler方程离散化\n",
    "        dx = (-y_state - z_state + i_x.squeeze(1)) * self.dt\n",
    "        dy = (x_state + self.a * y_state + i_y.squeeze(1)) * self.dt\n",
    "        dz = (self.b + z_state * (x_state - self.c) + i_z.squeeze(1)) * self.dt\n",
    "        \n",
    "        # 更新状态\n",
    "        new_x = x_state + dx\n",
    "        new_y = y_state + dy\n",
    "        new_z = z_state + dz\n",
    "        \n",
    "        # 组合新状态\n",
    "        new_hx = torch.stack([new_x, new_y, new_z], dim=1)\n",
    "        \n",
    "        # 计算Lyapunov损失\n",
    "        le_loss = self.le_reg(new_hx)\n",
    "        \n",
    "        return new_hx, le_loss\n",
    "\n",
    "# Lyapunov正则化器\n",
    "class LyapunovRegularizer(nn.Module):\n",
    "    def __init__(self, target_le=0.05, reg_strength=0.1):\n",
    "        super().__init__()\n",
    "        self.target_le = target_le\n",
    "        self.reg_strength = reg_strength\n",
    "        self.le_history = []\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # 使用雅可比矩阵的谱范数作为LE的近似\n",
    "        with torch.enable_grad():\n",
    "            batch_size, _, hidden_size = state.shape\n",
    "            state = state.detach().requires_grad_(True)\n",
    "            \n",
    "            perturbation = 1e-6 * torch.randn_like(state)\n",
    "            \n",
    "            dx = -state[:, 1] - state[:, 2]\n",
    "            dy = state[:, 0] + 0.2 * state[:, 1]\n",
    "            dz = 0.2 + state[:, 2] * (state[:, 0] - 5.7)\n",
    "            \n",
    "            jvp_dx = torch.autograd.grad(dx, state, grad_outputs=perturbation, \n",
    "                                        retain_graph=True, create_graph=False)[0]\n",
    "            jvp_dy = torch.autograd.grad(dy, state, grad_outputs=perturbation, \n",
    "                                        retain_graph=True, create_graph=False)[0]\n",
    "            jvp_dz = torch.autograd.grad(dz, state, grad_outputs=perturbation, \n",
    "                                        retain_graph=True, create_graph=False)[0]\n",
    "            \n",
    "            jvp = torch.stack([jvp_dx, jvp_dy, jvp_dz], dim=1)\n",
    "            \n",
    "            norm = torch.norm(jvp, dim=(1, 2))\n",
    "            le_approx = torch.log(norm) / 1.0\n",
    "        \n",
    "        current_le = le_approx.mean().item()\n",
    "        self.le_history.append(current_le)\n",
    "        \n",
    "        reg_loss = self.reg_strength * torch.abs(le_approx.mean() - self.target_le)\n",
    "        \n",
    "        return reg_loss\n",
    "    \n",
    "class ChaoticSpeechNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n",
    "        super(ChaoticSpeechNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 输入投影层\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Rossler混沌层\n",
    "        self.chaos_cells = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.chaos_cells.append(RosslerCell(hidden_size, hidden_size))\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc = nn.Linear(3 * hidden_size, num_classes)\n",
    "        \n",
    "        # 层归一化\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # 注意力机制\n",
    "        self.attention = nn.Linear(3 * hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        le_loss_total = 0.0\n",
    "        \n",
    "        # 初始化隐藏状态\n",
    "        hx = None\n",
    "        all_states = []\n",
    "        \n",
    "        # 处理序列\n",
    "        for t in range(seq_len):\n",
    "            # 输入投影\n",
    "            x_t = self.input_proj(x[:, t, :])\n",
    "            \n",
    "            # 通过所有混沌层\n",
    "            for i, cell in enumerate(self.chaos_cells):\n",
    "                if i == 0:\n",
    "                    input_t = x_t\n",
    "                else:\n",
    "                    input_t = h_state\n",
    "                \n",
    "                hx, le_loss = cell(input_t, hx)\n",
    "                le_loss_total += le_loss\n",
    "                h_state = hx[:, -1, :]\n",
    "                h_state = self.ln(h_state)\n",
    "            \n",
    "            all_states.append(hx)\n",
    "        \n",
    "        all_states = torch.stack(all_states, dim=0)\n",
    "        flat_states = all_states.view(seq_len, batch_size, -1)\n",
    "        \n",
    "        # 注意力加权\n",
    "        attn_weights = torch.softmax(self.attention(flat_states).squeeze(-1), dim=0)\n",
    "        context = torch.sum(attn_weights.unsqueeze(-1) * flat_states, dim=0)\n",
    "        \n",
    "        # 分类\n",
    "        output = self.fc(context)\n",
    "        \n",
    "        return output, le_loss_total / seq_len\n",
    "\n",
    "# 2. 传统LSTM模型\n",
    "class StandardLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=2):\n",
    "        super(StandardLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # 注意力机制\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # 层归一化\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 初始化隐藏状态\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM前向传播\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.ln(out)\n",
    "        \n",
    "        # 注意力机制\n",
    "        attn_weights = torch.softmax(self.attention(out).squeeze(-1), dim=1)\n",
    "        context = torch.sum(attn_weights.unsqueeze(-1) * out, dim=1)\n",
    "        \n",
    "        # 分类\n",
    "        output = self.fc(context)\n",
    "        \n",
    "        return output, torch.tensor(0.0)  # 返回0作为LE损失占位符\n",
    "\n",
    "# 训练函数 (适配两种模型)\n",
    "def train_model(model, model_name, dataloader, test_dataloader, num_epochs=30, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'test_acc': [], \n",
    "        'time_per_epoch': [], 'le_history': []\n",
    "    }\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs, le_loss = model(inputs)\n",
    "            ce_loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 混沌模型有额外的LE损失\n",
    "            if 'Chaotic' in model_name:\n",
    "                loss = ce_loss + le_loss\n",
    "            else:\n",
    "                loss = ce_loss\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 统计\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # 记录LE历史（仅混沌模型）\n",
    "            if 'Chaotic' in model_name:\n",
    "                history['le_history'].append(le_loss.item())\n",
    "            \n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f'{model_name} - Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], '\n",
    "                      f'Loss: {loss.item():.4f}, CE: {ce_loss.item():.4f}')\n",
    "        \n",
    "        # 计算训练精度\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_loss = running_loss / len(dataloader)\n",
    "        train_acc = 100 * correct / total\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['time_per_epoch'].append(epoch_time)\n",
    "        \n",
    "        # 在测试集上评估\n",
    "        test_acc = evaluate_model(model, test_dataloader)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(test_acc)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), f'best_{model_name.lower().replace(\" \", \"_\")}_model.pth')\n",
    "            print(f\"保存最佳模型，测试精度: {test_acc:.2f}%\")\n",
    "        \n",
    "        print(f'{model_name} - Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Time: {epoch_time:.1f}s, Train Loss: {train_loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 评估函数\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "# 结果可视化\n",
    "def plot_results(history, model_name, save_prefix):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.title(f'{model_name} - Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['test_acc'], label='Test Accuracy')\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    if model_name == 'Chaotic RNN':\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(history['le_history'])\n",
    "        plt.title('Lyapunov Exponent History')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('LE Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        window_size = 100\n",
    "        le_ma = np.convolve(history['le_history'], np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(le_ma)\n",
    "        plt.title(f'Lyapunov Exponent (Moving Avg, window={window_size})')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Smoothed LE')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_prefix}_training_results.png')\n",
    "    print(f\"{model_name} 训练结果图已保存至: {save_prefix}_training_results.png\")\n",
    "    plt.show()\n",
    "\n",
    "# 混淆矩阵可视化\n",
    "def plot_confusion_matrix(model, dataloader, class_names, model_name, save_path):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # 可视化\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"混淆矩阵已保存至: {save_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印分类报告\n",
    "    print(f\"\\n{model_name} 分类报告:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 参数配置\n",
    "    config = {\n",
    "        'base_path': 'F:/F/LifeLongLearning/TUNI/Thesis/Code/archive',\n",
    "        'batch_size': 32,\n",
    "        'max_length': 500,\n",
    "        'n_mfcc': 40,\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'lr': 0.001,\n",
    "        'num_epochs': 30,\n",
    "        'test_size': 0.2,\n",
    "        'seed': 42,\n",
    "        'actor_ids': list(range(1, 25))\n",
    "    }\n",
    "    \n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(config['seed'])\n",
    "    np.random.seed(config['seed'])\n",
    "    \n",
    "    # 加载数据集\n",
    "    print(\"加载数据集...\")\n",
    "    dataset = SpeechEmotionDataset(\n",
    "        base_path=config['base_path'], \n",
    "        max_length=config['max_length'],\n",
    "        n_mfcc=config['n_mfcc'],\n",
    "        actor_ids=config['actor_ids']\n",
    "    )\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(dataset)), \n",
    "        test_size=config['test_size'], \n",
    "        random_state=config['seed'],\n",
    "        stratify=dataset.labels_idx\n",
    "    )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    class_names = list(dataset.emotion_to_idx.keys())\n",
    "    \n",
    "    # 创建并训练混沌模型\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"训练混沌神经网络\")\n",
    "    print(\"=\"*50)\n",
    "    chaotic_model = ChaoticSpeechNet(\n",
    "        input_size=config['n_mfcc'],\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_classes=dataset.num_classes,\n",
    "        num_layers=config['num_layers']\n",
    "    ).to(device)\n",
    "    \n",
    "    chaotic_history = train_model(\n",
    "        chaotic_model,\n",
    "        \"Chaotic RNN\",\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        num_epochs=config['num_epochs'],\n",
    "        lr=config['lr']\n",
    "    )\n",
    "    \n",
    "    # 可视化混沌模型结果\n",
    "    plot_results(chaotic_history, \"Chaotic RNN\", \"chaotic\")\n",
    "    plot_confusion_matrix(chaotic_model, test_dataloader, class_names, \n",
    "                         \"Chaotic RNN\", \"chaotic_confusion_matrix.png\")\n",
    "    \n",
    "    # 创建并训练LSTM模型\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"训练标准LSTM模型\")\n",
    "    print(\"=\"*50)\n",
    "    lstm_model = StandardLSTM(\n",
    "        input_size=config['n_mfcc'],\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_classes=dataset.num_classes,\n",
    "        num_layers=config['num_layers']\n",
    "    ).to(device)\n",
    "    \n",
    "    lstm_history = train_model(\n",
    "        lstm_model,\n",
    "        \"Standard LSTM\",\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        num_epochs=config['num_epochs'],\n",
    "        lr=config['lr']\n",
    "    )\n",
    "    \n",
    "    # 可视化LSTM模型结果\n",
    "    plot_results(lstm_history, \"Standard LSTM\", \"lstm\")\n",
    "    plot_confusion_matrix(lstm_model, test_dataloader, class_names, \n",
    "                         \"Standard LSTM\", \"lstm_confusion_matrix.png\")\n",
    "    \n",
    "    # 横向对比结果\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # 精度对比\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(chaotic_history['test_acc'], 'r-', label='Chaotic RNN Test Acc')\n",
    "    plt.plot(lstm_history['test_acc'], 'b-', label='LSTM Test Acc')\n",
    "    plt.plot(chaotic_history['train_acc'], 'r--', label='Chaotic RNN Train Acc')\n",
    "    plt.plot(lstm_history['train_acc'], 'b--', label='LSTM Train Acc')\n",
    "    plt.title('Accuracy Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 损失对比\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(chaotic_history['train_loss'], 'r-', label='Chaotic RNN')\n",
    "    plt.plot(lstm_history['train_loss'], 'b-', label='LSTM')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 训练时间对比\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(['Chaotic RNN', 'LSTM'], \n",
    "            [np.mean(chaotic_history['time_per_epoch']), \n",
    "            np.mean(lstm_history['time_per_epoch'])])\n",
    "    plt.title('Average Training Time per Epoch')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    \n",
    "    # 最终性能对比\n",
    "    chaotic_final_acc = chaotic_history['test_acc'][-1]\n",
    "    lstm_final_acc = lstm_history['test_acc'][-1]\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar(['Chaotic RNN', 'LSTM'], [chaotic_final_acc, lstm_final_acc])\n",
    "    plt.title('Final Test Accuracy')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(min(chaotic_final_acc, lstm_final_acc)-5, max(chaotic_final_acc, lstm_final_acc)+5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    print(\"模型对比图已保存至: model_comparison.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印最终对比结果\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"模型性能对比总结\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"混沌神经网络 最终测试精度: {chaotic_final_acc:.2f}%\")\n",
    "    print(f\"标准LSTM模型 最终测试精度: {lstm_final_acc:.2f}%\")\n",
    "    print(f\"平均每轮训练时间 - 混沌模型: {np.mean(chaotic_history['time_per_epoch']):.1f}s, LSTM: {np.mean(lstm_history['time_per_epoch']):.1f}s\")\n",
    "    \n",
    "    # 情感类别性能对比\n",
    "    chaotic_preds = []\n",
    "    lstm_preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # 混沌模型预测\n",
    "            chaotic_outputs, _ = chaotic_model(inputs)\n",
    "            _, chaotic_pred = torch.max(chaotic_outputs, 1)\n",
    "            chaotic_preds.extend(chaotic_pred.cpu().numpy())\n",
    "            \n",
    "            # LSTM模型预测\n",
    "            lstm_outputs, _ = lstm_model(inputs)\n",
    "            _, lstm_pred = torch.max(lstm_outputs, 1)\n",
    "            lstm_preds.extend(lstm_pred.cpu().numpy())\n",
    "            \n",
    "            true_labels.extend(labels.numpy())\n",
    "    \n",
    "    # 计算每个类别的准确率\n",
    "    chaotic_class_acc = []\n",
    "    lstm_class_acc = []\n",
    "    \n",
    "    for i, emotion in enumerate(class_names):\n",
    "        indices = [j for j, label in enumerate(true_labels) if label == i]\n",
    "        \n",
    "        chaotic_correct = sum(1 for j in indices if chaotic_preds[j] == i)\n",
    "        lstm_correct = sum(1 for j in indices if lstm_preds[j] == i)\n",
    "        \n",
    "        chaotic_acc = chaotic_correct / len(indices) * 100 if len(indices) > 0 else 0\n",
    "        lstm_acc = lstm_correct / len(indices) * 100 if len(indices) > 0 else 0\n",
    "        \n",
    "        chaotic_class_acc.append(chaotic_acc)\n",
    "        lstm_class_acc.append(lstm_acc)\n",
    "    \n",
    "    # 绘制类别性能对比\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, chaotic_class_acc, width, label='Chaotic RNN')\n",
    "    plt.bar(x + width/2, lstm_class_acc, width, label='Standard LSTM')\n",
    "    \n",
    "    plt.xlabel('Emotion Class')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy by Emotion Class')\n",
    "    plt.xticks(x, class_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_performance_comparison.png')\n",
    "    print(\"类别性能对比图已保存至: class_performance_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc448c-0041-4f9e-8797-ab0d70236bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-VirEnvir] *",
   "language": "python",
   "name": "conda-env-.conda-VirEnvir-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
