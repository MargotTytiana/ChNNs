import os
import time
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional, Union
from tqdm import tqdm
import json
import logging

# Import custom modules
# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_loader import create_dataloaders
from speaker_model import (
    ChaoticSpeakerRecognitionSystem,
    SpeakerRecognitionLoss,
    evaluate_speaker_recognition,
    evaluate_speaker_verification,
    plot_embeddings_2d
)
from config import get_config


def setup_logging(log_dir: str) -> None:
    """
    Set up logging configuration

    è®¾ç½®æ—¥å¿—é…ç½®

    Args:
        log_dir (str): Directory to save log files
                      ä¿å­˜æ—¥å¿—æ–‡ä»¶çš„ç›®å½•
    """
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    log_file = os.path.join(log_dir, f"training_{time.strftime('%Y%m%d_%H%M%S')}.log")

    # Configure logging
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )


def save_checkpoint(
        model: nn.Module,
        optimizer: optim.Optimizer,
        epoch: int,
        loss: float,
        accuracy: float,
        checkpoint_dir: str,
        is_best: bool = False
) -> None:
    """
    Save model checkpoint

    ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹

    Args:
        model (nn.Module): Model to save
                          è¦ä¿å­˜çš„æ¨¡å‹
        optimizer (optim.Optimizer): Optimizer state
                                    ä¼˜åŒ–å™¨çŠ¶æ€
        epoch (int): Current epoch
                    å½“å‰è½®æ¬¡
        loss (float): Current loss
                     å½“å‰æŸå¤±
        accuracy (float): Current accuracy
                         å½“å‰å‡†ç¡®ç‡
        checkpoint_dir (str): Directory to save checkpoint
                             ä¿å­˜æ£€æŸ¥ç‚¹çš„ç›®å½•
        is_best (bool): Whether this is the best model so far
                       è¿™æ˜¯å¦æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¥½çš„æ¨¡å‹
    """
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'accuracy': accuracy
    }

    # Save regular checkpoint
    # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
    checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch}.pt")
    torch.save(checkpoint, checkpoint_path)

    # Save latest checkpoint (overwrite)
    # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ï¼ˆè¦†ç›–ï¼‰
    latest_path = os.path.join(checkpoint_dir, "checkpoint_latest.pt")
    torch.save(checkpoint, latest_path)

    # If this is the best model, save it separately
    # å¦‚æœè¿™æ˜¯æœ€å¥½çš„æ¨¡å‹ï¼Œå•ç‹¬ä¿å­˜
    if is_best:
        best_path = os.path.join(checkpoint_dir, "checkpoint_best.pt")
        torch.save(checkpoint, best_path)

    logging.info(f"Checkpoint saved at {checkpoint_path}")


def load_checkpoint(
        model: nn.Module,
        optimizer: Optional[optim.Optimizer],
        checkpoint_path: str
) -> Tuple[nn.Module, Optional[optim.Optimizer], int, float, float]:
    """
    Load model checkpoint

    åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹

    Args:
        model (nn.Module): Model to load weights into
                          è¦åŠ è½½æƒé‡çš„æ¨¡å‹
        optimizer (optim.Optimizer, optional): Optimizer to load state into
                                             è¦åŠ è½½çŠ¶æ€çš„ä¼˜åŒ–å™¨
        checkpoint_path (str): Path to checkpoint file
                              æ£€æŸ¥ç‚¹æ–‡ä»¶çš„è·¯å¾„

    Returns:
        Tuple: Updated model, optimizer, epoch, loss, and accuracy
               æ›´æ–°çš„æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è½®æ¬¡ã€æŸå¤±å’Œå‡†ç¡®ç‡
    """
    checkpoint = torch.load(checkpoint_path)

    model.load_state_dict(checkpoint['model_state_dict'])

    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    accuracy = checkpoint.get('accuracy', 0.0)  # Handle older checkpoints without accuracy

    logging.info(f"Loaded checkpoint from epoch {epoch} with loss {loss:.4f} and accuracy {accuracy:.4f}")

    return model, optimizer, epoch, loss, accuracy


def train_epoch(
        model: nn.Module,
        dataloader: DataLoader,
        criterion: nn.Module,
        optimizer: optim.Optimizer,
        device: torch.device,
        epoch: int,
        log_interval: int = 10
) -> Tuple[float, float]:
    """
    Train for one epoch

    è®­ç»ƒä¸€ä¸ªè½®æ¬¡

    Args:
        model (nn.Module): Model to train
                          è¦è®­ç»ƒçš„æ¨¡å‹
        dataloader (DataLoader): Training data loader
                                è®­ç»ƒæ•°æ®åŠ è½½å™¨
        criterion (nn.Module): Loss function
                              æŸå¤±å‡½æ•°
        optimizer (optim.Optimizer): Optimizer
                                    ä¼˜åŒ–å™¨
        device (torch.device): Device to train on
                              è®­ç»ƒè®¾å¤‡
        epoch (int): Current epoch number
                    å½“å‰è½®æ¬¡ç¼–å·
        log_interval (int): How often to log progress
                           å¤šä¹…è®°å½•ä¸€æ¬¡è¿›åº¦

    Returns:
        Tuple[float, float]: Average loss and accuracy for the epoch
                            è¯¥è½®æ¬¡çš„å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    """
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}")

    for batch_idx, batch in enumerate(progress_bar):
        # Get data
        # è·å–æ•°æ®
        features = batch['audio'].to(device)
        labels = batch['label'].to(device)

        # ======== ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯ ========
        if epoch == 0 and batch_idx == 0:
            print(f"Input audio shape: {features.shape}")
            print(f"Input audio range: {features.min().item():.4f} to {features.max().item():.4f}")
            print(f"Input audio mean: {features.mean().item():.4f}, std: {features.std().item():.4f}")
        # ================================

        # Zero the parameter gradients
        # æ¸…é›¶å‚æ•°æ¢¯åº¦
        optimizer.zero_grad()

        # Forward pass
        # å‰å‘ä¼ æ’­
        outputs = model(features, labels=labels, mode='identification')

        # Calculate loss
        # è®¡ç®—æŸå¤±
        losses = criterion(outputs, labels)
        loss = losses['total_loss']

        # ======== ğŸ” è°ƒè¯•æ‰“å°ï¼Œåªåœ¨ç¬¬ä¸€ä¸ª epoch çš„ç¬¬ä¸€ä¸ª batch æ‰“å° ========
        if epoch == 0 and batch_idx == 0:
            print("\n=== DEBUG INFO (First Batch) ===")
            print("Output logits shape:", outputs['logits'].shape)  # åº”è¯¥æ˜¯ [batch_size, num_speakers]
            print("Labels min/max:", labels.min().item(), labels.max().item())  # åº”è¯¥åœ¨ 0..num_speakers-1
            print("Loss (total):", loss.item())
            print("Predictions (first 10):", torch.argmax(outputs['logits'], dim=1)[:10].cpu().numpy())
            print("Labels      (first 10):", labels[:10].cpu().numpy())
            print("===============================\n")
            print(f"Model output logits shape: {outputs['logits'].shape}")
            print(f"Model output embeddings shape: {outputs['embeddings'].shape}")
            print(f"Labels: {labels[:10].cpu().numpy()}")
        # ============================================================

        # Backward pass and optimize
        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        loss.backward()
        optimizer.step()

        # Update statistics
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        running_loss += loss.item()
        _, predicted = torch.max(outputs['logits'], 1)
        batch_total = labels.size(0)
        batch_correct = (predicted == labels).sum().item()
        total += batch_total
        correct += batch_correct

        # Update progress bar
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'loss': loss.item(),
            'acc': batch_correct / batch_total
        })

        # Log progress
        # è®°å½•è¿›åº¦
        if (batch_idx + 1) % log_interval == 0:
            logging.info(
                f"Epoch {epoch} [{batch_idx + 1}/{len(dataloader)}] - Loss: {loss.item():.4f}, Acc: {batch_correct / batch_total:.4f}")

    # Calculate epoch statistics
    # è®¡ç®—è½®æ¬¡ç»Ÿè®¡ä¿¡æ¯
    epoch_loss = running_loss / len(dataloader)
    epoch_accuracy = correct / total if total > 0 else 0

    logging.info(f"Epoch {epoch} completed - Avg Loss: {epoch_loss:.4f}, Avg Acc: {epoch_accuracy:.4f}")

    return epoch_loss, epoch_accuracy


def validate(
        model: nn.Module,
        dataloader: DataLoader,
        criterion: nn.Module,
        device: torch.device,
        epoch: int
) -> Tuple[float, float]:
    """
    Validate the model

    éªŒè¯æ¨¡å‹

    Args:
        model (nn.Module): Model to validate
                          è¦éªŒè¯çš„æ¨¡å‹
        dataloader (DataLoader): Validation data loader
                                éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion (nn.Module): Loss function
                              æŸå¤±å‡½æ•°
        device (torch.device): Device to validate on
                              éªŒè¯è®¾å¤‡
        epoch (int): Current epoch number
                    å½“å‰è½®æ¬¡ç¼–å·

    Returns:
        Tuple[float, float]: Validation loss and accuracy
                            éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡
    """
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc=f"Validation {epoch}")

        for batch in progress_bar:
            # Get data
            # è·å–æ•°æ®
            features = batch['audio'].to(device)
            labels = batch['label'].to(device)

            # Forward pass
            # å‰å‘ä¼ æ’­
            outputs = model(features, labels=labels, mode='identification')

            # Calculate loss
            # è®¡ç®—æŸå¤±
            losses = criterion(outputs, labels)
            loss = losses['total_loss']

            # Update statistics
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            running_loss += loss.item()
            _, predicted = torch.max(outputs['logits'], 1)
            batch_total = labels.size(0)
            batch_correct = (predicted == labels).sum().item()
            total += batch_total
            correct += batch_correct

            # Update progress bar
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': loss.item(),
                'acc': batch_correct / batch_total
            })

    # Calculate validation statistics
    # è®¡ç®—éªŒè¯ç»Ÿè®¡ä¿¡æ¯
    val_loss = running_loss / len(dataloader)
    val_accuracy = correct / total if total > 0 else 0

    logging.info(f"Validation Epoch {epoch} - Loss: {val_loss:.4f}, Acc: {val_accuracy:.4f}")

    return val_loss, val_accuracy


def extract_embeddings(
        model: nn.Module,
        dataloader: DataLoader,
        device: torch.device
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Extract speaker embeddings from the model

    ä»æ¨¡å‹ä¸­æå–è¯´è¯äººåµŒå…¥

    Args:
        model (nn.Module): Trained model
                          è®­ç»ƒå¥½çš„æ¨¡å‹
        dataloader (DataLoader): Data loader
                                æ•°æ®åŠ è½½å™¨
        device (torch.device): Device to run on
                              è¿è¡Œè®¾å¤‡

    Returns:
        Tuple[np.ndarray, np.ndarray]: Speaker embeddings and corresponding labels
                                      è¯´è¯äººåµŒå…¥å’Œå¯¹åº”çš„æ ‡ç­¾
    """
    model.eval()
    all_embeddings = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Extracting embeddings"):
            # Get data
            # è·å–æ•°æ®
            features = batch['audio'].to(device)
            labels = batch['label']

            # Forward pass to get embeddings
            # å‰å‘ä¼ æ’­è·å–åµŒå…¥
            outputs = model(features, mode='embedding')
            embeddings = outputs['embeddings']

            # Store embeddings and labels
            # å­˜å‚¨åµŒå…¥å’Œæ ‡ç­¾
            all_embeddings.append(embeddings.cpu().numpy())
            all_labels.append(labels.numpy())

    # Concatenate all batches
    # è¿æ¥æ‰€æœ‰æ‰¹æ¬¡
    all_embeddings = np.vstack(all_embeddings)
    all_labels = np.concatenate(all_labels)

    return all_embeddings, all_labels


def train_model(config: Dict) -> None:
    """
    Train the speaker recognition model

    è®­ç»ƒè¯´è¯äººè¯†åˆ«æ¨¡å‹

    Args:
        config (Dict): Configuration dictionary
                      é…ç½®å­—å…¸
    """
    # Set up logging
    # è®¾ç½®æ—¥å¿—
    setup_logging(config['log_dir'])
    logging.info("Starting training with config:")
    logging.info(json.dumps(config, indent=2))

    # Set device
    # è®¾ç½®è®¾å¤‡
    device = torch.device(config['device'])
    logging.info(f"Using device: {device}")

    # Set random seeds for reproducibility
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    torch.manual_seed(config['seed'])
    np.random.seed(config['seed'])
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config['seed'])

    # Create data loaders
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    logging.info("Creating data loaders...")
    train_loader, dev_loader, test_loader, speaker_to_idx = create_dataloaders(
        train_dir=config['train_dir'],
        dev_dir=config['dev_dir'],
        test_dir=config['test_dir'],
        batch_size=config['batch_size'],
        segment_length=config['segment_length'],
        sampling_rate=config['sampling_rate'],
        num_workers=config['num_workers']
    )

    num_speakers = len(speaker_to_idx)
    logging.info(f"Number of speakers: {num_speakers}")

    # Create model
    # åˆ›å»ºæ¨¡å‹
    logging.info("Creating model...")
    model = ChaoticSpeakerRecognitionSystem(
        chaotic_feature_dim=config['chaotic_feature_dim'],
        chaotic_dim=config['chaotic_dim'],
        trajectory_points=config['trajectory_points'],
        embedding_dim=config['embedding_dim'],
        num_speakers=num_speakers,
        use_chaotic_embedding=config['use_chaotic_embedding'],
        use_attractor_pooling=config['use_attractor_pooling'],
        system_type=config['system_type']
    )
    model = model.to(device)

    # Create loss function and optimizer
    # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = SpeakerRecognitionLoss(
        ce_weight=config['ce_weight'],
        triplet_weight=config['triplet_weight'],
        margin=config['triplet_margin']
    )

    optimizer = optim.Adam(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )

    # Create learning rate scheduler
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=config['lr_patience'],
        # verbose=True
    )

    # Resume from checkpoint if specified
    # å¦‚æœæŒ‡å®šï¼Œä»æ£€æŸ¥ç‚¹æ¢å¤
    start_epoch = 0
    best_val_accuracy = 0.0

    if config['resume_checkpoint']:
        if os.path.isfile(config['resume_checkpoint']):
            logging.info(f"Loading checkpoint: {config['resume_checkpoint']}")
            model, optimizer, start_epoch, _, best_val_accuracy = load_checkpoint(
                model, optimizer, config['resume_checkpoint']
            )
            start_epoch += 1  # Start from the next epoch
        else:
            logging.warning(f"No checkpoint found at {config['resume_checkpoint']}")

    # Training loop
    # è®­ç»ƒå¾ªç¯
    logging.info("Starting training...")

    # Lists to store metrics for plotting
    # ç”¨äºç»˜å›¾çš„æŒ‡æ ‡åˆ—è¡¨
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    for epoch in range(start_epoch, config['num_epochs']):
        # Train for one epoch
        # è®­ç»ƒä¸€ä¸ªè½®æ¬¡
        train_loss, train_accuracy = train_epoch(
            model=model,
            dataloader=train_loader,
            criterion=criterion,
            optimizer=optimizer,
            device=device,
            epoch=epoch,
            log_interval=config['log_interval']
        )

        # Validate
        # éªŒè¯
        val_loss, val_accuracy = validate(
            model=model,
            dataloader=dev_loader,
            criterion=criterion,
            device=device,
            epoch=epoch
        )

        # Update learning rate
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_loss)

        # Store metrics for plotting
        # å­˜å‚¨æŒ‡æ ‡ç”¨äºç»˜å›¾
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        # Check if this is the best model
        # æ£€æŸ¥è¿™æ˜¯å¦æ˜¯æœ€å¥½çš„æ¨¡å‹
        is_best = val_accuracy > best_val_accuracy
        if is_best:
            best_val_accuracy = val_accuracy

        # Save checkpoint
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % config['checkpoint_interval'] == 0 or is_best:
            save_checkpoint(
                model=model,
                optimizer=optimizer,
                epoch=epoch,
                loss=val_loss,
                accuracy=val_accuracy,
                checkpoint_dir=config['checkpoint_dir'],
                is_best=is_best
            )

        # Plot and save training curves
        # ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæ›²çº¿
        if (epoch + 1) % config['plot_interval'] == 0:
            plot_training_curves(
                train_losses=train_losses,
                val_losses=val_losses,
                train_accuracies=train_accuracies,
                val_accuracies=val_accuracies,
                save_dir=config['plot_dir'],
                epoch=epoch
            )

    # Final evaluation on test set
    # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
    logging.info("Training completed. Evaluating on test set...")

    # Load best model for evaluation
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°
    best_model_path = os.path.join(config['checkpoint_dir'], "checkpoint_best.pt")
    if os.path.isfile(best_model_path):
        model, _, _, _, _ = load_checkpoint(model, None, best_model_path)

    # Evaluate on test set
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_metrics = evaluate_speaker_recognition(model, test_loader, device)

    logging.info("Test set evaluation results:")
    logging.info(f"Accuracy: {test_metrics['accuracy']:.4f}")

    # Extract and visualize embeddings
    # æå–å¹¶å¯è§†åŒ–åµŒå…¥
    if config['visualize_embeddings']:
        logging.info("Extracting embeddings for visualization...")
        embeddings, labels = extract_embeddings(model, test_loader, device)

        # Save embeddings
        # ä¿å­˜åµŒå…¥
        embeddings_dir = os.path.join(config['output_dir'], 'embeddings')
        if not os.path.exists(embeddings_dir):
            os.makedirs(embeddings_dir)

        np.save(os.path.join(embeddings_dir, 'embeddings.npy'), embeddings)
        np.save(os.path.join(embeddings_dir, 'labels.npy'), labels)

        # Visualize embeddings
        # å¯è§†åŒ–åµŒå…¥
        logging.info("Visualizing embeddings...")
        plot_embeddings_2d(
            embeddings=embeddings,
            labels=labels,
            title="Speaker Embeddings (t-SNE Visualization)"
        )
        plt.savefig(os.path.join(config['plot_dir'], 'embeddings_visualization.png'))

    logging.info("Training and evaluation completed.")


def plot_training_curves(
        train_losses: List[float],
        val_losses: List[float],
        train_accuracies: List[float],
        val_accuracies: List[float],
        save_dir: str,
        epoch: int
) -> None:
    """
    Plot and save training curves

    ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæ›²çº¿

    Args:
        train_losses (List[float]): Training losses
                                   è®­ç»ƒæŸå¤±
        val_losses (List[float]): Validation losses
                                 éªŒè¯æŸå¤±
        train_accuracies (List[float]): Training accuracies
                                       è®­ç»ƒå‡†ç¡®ç‡
        val_accuracies (List[float]): Validation accuracies
                                     éªŒè¯å‡†ç¡®ç‡
        save_dir (str): Directory to save plots
                       ä¿å­˜å›¾è¡¨çš„ç›®å½•
        epoch (int): Current epoch
                    å½“å‰è½®æ¬¡
    """
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # Create figure with two subplots
    # åˆ›å»ºå¸¦æœ‰ä¸¤ä¸ªå­å›¾çš„å›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Plot losses
    # ç»˜åˆ¶æŸå¤±
    epochs = range(1, len(train_losses) + 1)
    ax1.plot(epochs, train_losses, 'b-', label='Training Loss')
    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)

    # Plot accuracies
    # ç»˜åˆ¶å‡†ç¡®ç‡
    ax2.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')
    ax2.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')
    ax2.set_title('Training and Validation Accuracy')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True)

    # Adjust layout and save
    # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'training_curves_epoch_{epoch}.png'))
    plt.close()


def main():
    """
    Main function

    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description="Train Speaker Recognition Model")

    parser.add_argument('--config', type=str, default='configs/default.json',
                        help='Path to configuration file')
    parser.add_argument('--mode', type=str, default='train',
                        choices=['train', 'evaluate', 'extract_embeddings'],
                        help='Operation mode')
    parser.add_argument('--checkpoint', type=str, default=None,
                        help='Path to checkpoint for evaluation or embedding extraction')

    args = parser.parse_args()

    # Load configuration
    # åŠ è½½é…ç½®
    config = get_config(args.config)

    # Update config with command line arguments
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®
    if args.checkpoint:
        config['resume_checkpoint'] = args.checkpoint

    # Execute based on mode
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œ
    if args.mode == 'train':
        train_model(config)

    elif args.mode == 'evaluate':
        # Set up logging
        # è®¾ç½®æ—¥å¿—
        setup_logging(config['log_dir'])

        # Set device
        # è®¾ç½®è®¾å¤‡
        device = torch.device(config['device'])

        # Create data loaders
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        _, _, test_loader, speaker_to_idx = create_dataloaders(
            train_dir=config['train_dir'],
            dev_dir=config['dev_dir'],
            test_dir=config['test_dir'],
            batch_size=config['batch_size'],
            segment_length=config['segment_length'],
            sampling_rate=config['sampling_rate'],
            num_workers=config['num_workers']
        )

        # Create model
        # åˆ›å»ºæ¨¡å‹
        model = ChaoticSpeakerRecognitionSystem(
            chaotic_feature_dim=config['chaotic_feature_dim'],
            chaotic_dim=config['chaotic_dim'],
            trajectory_points=config['trajectory_points'],
            embedding_dim=config['embedding_dim'],
            num_speakers=len(speaker_to_idx),
            use_chaotic_embedding=config['use_chaotic_embedding'],
            use_attractor_pooling=config['use_attractor_pooling'],
            system_type=config['system_type']
        )
        model = model.to(device)

        # Load checkpoint
        # åŠ è½½æ£€æŸ¥ç‚¹
        if not args.checkpoint:
            logging.error("Checkpoint path must be provided for evaluation mode")
            return

        model, _, _, _, _ = load_checkpoint(model, None, args.checkpoint)

        # Evaluate
        # è¯„ä¼°
        logging.info("Evaluating model on test set...")
        test_metrics = evaluate_speaker_recognition(model, test_loader, device)

        logging.info("Test set evaluation results:")
        logging.info(json.dumps(test_metrics, indent=2))

    elif args.mode == 'extract_embeddings':
        # Set up logging
        # è®¾ç½®æ—¥å¿—
        setup_logging(config['log_dir'])

        # Set device
        # è®¾ç½®è®¾å¤‡
        device = torch.device(config['device'])

        # Create data loaders
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        _, _, test_loader, speaker_to_idx = create_dataloaders(
            train_dir=config['train_dir'],
            dev_dir=config['dev_dir'],
            test_dir=config['test_dir'],
            batch_size=config['batch_size'],
            segment_length=config['segment_length'],
            sampling_rate=config['sampling_rate'],
            num_workers=config['num_workers']
        )

        # Create model
        # åˆ›å»ºæ¨¡å‹
        model = ChaoticSpeakerRecognitionSystem(
            chaotic_feature_dim=config['chaotic_feature_dim'],
            chaotic_dim=config['chaotic_dim'],
            trajectory_points=config['trajectory_points'],
            embedding_dim=config['embedding_dim'],
            num_speakers=len(speaker_to_idx),
            use_chaotic_embedding=config['use_chaotic_embedding'],
            use_attractor_pooling=config['use_attractor_pooling'],
            system_type=config['system_type']
        )
        model = model.to(device)

        # Load checkpoint
        # åŠ è½½æ£€æŸ¥ç‚¹
        if not args.checkpoint:
            logging.error("Checkpoint path must be provided for embedding extraction mode")
            return

        model, _, _, _, _ = load_checkpoint(model, None, args.checkpoint)

        # Extract embeddings
        # æå–åµŒå…¥
        logging.info("Extracting embeddings...")
        embeddings, labels = extract_embeddings(model, test_loader, device)

        # Save embeddings
        # ä¿å­˜åµŒå…¥
        embeddings_dir = os.path.join(config['output_dir'], 'embeddings')
        if not os.path.exists(embeddings_dir):
            os.makedirs(embeddings_dir)

        np.save(os.path.join(embeddings_dir, 'embeddings.npy'), embeddings)
        np.save(os.path.join(embeddings_dir, 'labels.npy'), labels)

        logging.info(f"Embeddings saved to {embeddings_dir}")

        # Visualize embeddings
        # å¯è§†åŒ–åµŒå…¥
        if config['visualize_embeddings']:
            logging.info("Visualizing embeddings...")
            plot_embeddings_2d(
                embeddings=embeddings,
                labels=labels,
                title="Speaker Embeddings (t-SNE Visualization)"
            )

            plot_dir = config['plot_dir']
            if not os.path.exists(plot_dir):
                os.makedirs(plot_dir)

            plt.savefig(os.path.join(plot_dir, 'embeddings_visualization.png'))
            logging.info(f"Visualization saved to {os.path.join(plot_dir, 'embeddings_visualization.png')}")


if __name__ == "__main__":
    main()
